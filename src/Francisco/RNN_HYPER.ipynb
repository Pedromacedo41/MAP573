{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import math\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras import callbacks\n",
    "import pickle\n",
    "import talos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 18.49it/s]\n"
     ]
    }
   ],
   "source": [
    "PATH = r\"../../Data/completeLoad.csv\"\n",
    "df = pd.read_csv(PATH)\n",
    "df_h = df.set_index(['zone_id', 'year', 'month', 'day'])\n",
    "unfolded_df = {}\n",
    "for i, x in enumerate(tqdm(df_h.columns.tolist())):\n",
    "    col_tuple = df_h[x].to_dict()\n",
    "    for j, k in col_tuple.items():\n",
    "        unfolded_df[tuple(list(j) + [i+1])] = k\n",
    "\n",
    "n_df = pd.DataFrame.from_dict(unfolded_df, orient=\"index\").sort_index()\n",
    "m_index = pd.MultiIndex.from_tuples(unfolded_df.keys())\n",
    "mi_df = pd.DataFrame(unfolded_df.values(), m_index).sort_index()\n",
    "df_zone = np.array([mi_df.loc[(i+1)].values.astype(\"float32\").reshape(\n",
    "        len(mi_df.loc[(i+1)]),) for i in range(mi_df.index[-1][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples training set: 31660\n",
      "Number of samples test set: 7916\n"
     ]
    }
   ],
   "source": [
    "# use data for zone 1.\n",
    "data = df_zone[0].reshape(-1, 1)\n",
    "\n",
    "# normalize data with min max normalization.\n",
    "normalizer = MinMaxScaler(feature_range = (0, 1))\n",
    "dataset = normalizer.fit_transform(data)\n",
    "\n",
    "# Using 80% of data for training, 20% for validation.\n",
    "TRAINING_PERC = 0.80\n",
    "\n",
    "train_size = int(len(dataset) * TRAINING_PERC)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "print(\"Number of samples training set: \" + str((len(train))))\n",
    "print(\"Number of samples test set: \" + str((len(test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to read data.\n",
    "def create_dataset(dataset, window_size = 1):\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(dataset) - window_size - 1):\n",
    "        sample = dataset[i:(i + window_size), 0]\n",
    "        data_x.append(sample)\n",
    "        data_y.append(dataset[i + window_size, 0])\n",
    "    return(np.array(data_x), np.array(data_y))\n",
    "\n",
    "def create_model(train_X, train_Y, window_size = 1):\n",
    "    vanilla_rnn = Sequential()\n",
    "    vanilla_rnn.add(LSTM(5,input_shape = (1, window_size), ))\n",
    "    vanilla_rnn.add(Dense(1))\n",
    "    vanilla_rnn.compile(loss = \"mean_squared_error\", \n",
    "                  optimizer = \"adam\")\n",
    "    \n",
    "    return(vanilla_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_talos(train_X, train_Y, val_X, val_Y, params):\n",
    "    vanilla_rnn = Sequential()\n",
    "    \n",
    "    for i in range(max(params['layers']-1, 0)):\n",
    "        vanilla_rnn.add(LSTM(params['first_lstm'], return_sequences=True))\n",
    "\n",
    "    vanilla_rnn.add(LSTM(params['first_lstm'],input_shape = (1, window_size), activation='tanh', \n",
    "                         recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                         recurrent_initializer='orthogonal', bias_initializer='zeros'))\n",
    "    \n",
    "    vanilla_rnn.add(Dense(1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros'))\n",
    "    \n",
    "    vanilla_rnn.compile(params['optimizer'], loss=params['loss'])\n",
    "    \n",
    "    history = vanilla_rnn.fit(train_X, train_Y, epochs = params['epochs'], batch_size = params['batch_size'], \n",
    "                              verbose = 0,  validation_data=[val_X, val_Y])\n",
    "    \n",
    "    return history, vanilla_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window size trained seperately\n",
    "p = {'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "     'loss': ['mse'],\n",
    "     'batch_size': [32, 128, 1024],\n",
    "     'epochs': [5, 10, 20],\n",
    "     'layers' : [1, 2],\n",
    "     'first_lstm' : [4, 8, 16, 32]}\n",
    "\n",
    "scan_object = talos.Scan(train_X, train_Y, model=model_talos, params=p, fraction_limit=0.2, val_split=0.15, experiment_name='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing the results data frame\n",
    "scan_object.data.head()\n",
    "\n",
    "# accessing epoch entropy values for each round\n",
    "scan_object.learning_entropy\n",
    "\n",
    "# access the summary details\n",
    "scan_object.details\n",
    "\n",
    "# use Scan object as input\n",
    "analyze_object = talos.Analyze(scan_object)\n",
    "\n",
    "# access the dataframe with the results\n",
    "analyze_object.data\n",
    "\n",
    "# four dimensional bar grid\n",
    "analyze_object.plot_bars('layers', 'val_loss', 'window_size', 'optimizer')\n",
    "\n",
    "# four dimensional bar grid\n",
    "analyze_object.plot_bars('batch_size', 'val_loss', 'first_lstm', 'epochs')\n",
    "\n",
    "# heatmap correlation\n",
    "analyze_object.plot_corr('val_loss', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "iterator = [10, 20, 50, 100, 200, 500, 1000]\n",
    "for i in iterator:\n",
    "    window_size = i\n",
    "    train_X, train_Y = create_dataset(train, window_size)\n",
    "    test_X, test_Y = create_dataset(test, window_size)\n",
    "    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "    vanilla_rnn = create_model(train_X, train_Y, window_size)\n",
    "    history = vanilla_rnn.fit(train_X, train_Y, epochs = 1000, batch_size = 32, verbose = 1, validation_split=0.15, callbacks=[es])\n",
    "    losses.append(min(history.history['val_loss']))\n",
    "plt.plot(iterator, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
