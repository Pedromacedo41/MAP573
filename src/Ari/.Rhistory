## Autocorrelations
```{r ACF}
library(forecast)
p1 <- ggAcf(TS_sum, lag.max = 48) + labs(title="Autocorrelations for lags 48 hours")+theme_bw()
p2 <- ggAcf(TS_sum, lag.max = 24*35) + labs(title="Autocorrelations for lags 840 hours (35 days)")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
```
The autocorrelation decreases from lag 1 but then around 15, it starts increasing, reacking a peak at 24. This behaviour repeats itself every 24 hours, decreasing a little every time.
However, there are some intervals where it increases sligly (see bottom figure). Where the ACF increases slightly is the 6th and the 6th and the 7th lag.
From this we have some information that there is a autocorrelation for lags 24, 48,72,... and there is also some autocorrelation increase for lags $$6*24$$ and $$7*24$$, meaning that the same weekdays have are more correlated than different ones.
## Partial autocorrelation
To get the direct effect, we performed partial autocorrelation. Here it is easier to see what is going on. The first figure shows how autocorrelation drops, barely correlation around lag 10 but then goes up and is positively correlated for lags 15-16.
The correlation is however greatest at lag 24 (nagative correlation). This is consistent with what we saw earlier.
In the next figure, the lags are extended to 200.
It is clear that the PACF is the greatest at lag 24. However, every 24 lags it decreases up untill lags $$6*24$$ and $$7*24$$ (also onsistent with what we saw earlier).
Extending lags more, weekly correlation decreases.
There seems to be a seasonal period of 24 hours. Also, when lags are increased there seem to be another seasonal period of 7 days.
Now to partial autocorrelation.
```{r PACF}
p1 <- ggAcf(TS_sum, type = "partial")+theme_bw()+labs(title="PACF for 24 hours")
p2 <- ggAcf(TS_sum, type = "partial", lag.max = 24*7)+theme_bw()+labs(title="PACF for a week")
p3 <- ggAcf(TS_sum, type = "partial", lag.max = 24*7*4)+theme_bw()+labs(title="PACF for a month")
grid.arrange(p1, p2, p3, ncol = 2, layout_matrix = rbind(c(1,2), c(3,3)))
```
```{r topPCAF}
#Which PACF are the greatest
temp <- ggAcf(TS_sum, type = "partial", plot=F, lag.max = 24*7*4)
temp <- as.data.frame(temp$acf)
temp$idx  <- 1:nrow(temp)
colnames(temp) <- c("PACF", "IDX")
signs <- temp[c("IDX", "PACF")]
nosigns <- abs(signs)
maxPACF <- order(nosigns$PACF, decreasing=T)
maxPACF[1:15]
results <- signs[maxPACF,]
#Now check for 4 years, years seem to be less autocorrelated than within year
temp <- ggAcf(TS_sum, type = "partial", plot=F, lag.max = 24*7*4*12*2)
temp <- as.data.frame(temp$acf)
temp$idx  <- 1:nrow(temp)
colnames(temp) <- c("PACF", "IDX")
signs <- temp[c("IDX", "PACF")]
nosigns <- abs(signs)
maxPACF <- order(nosigns$PACF, decreasing=T)
maxPACF[1:15]
results <- signs[maxPACF,]
```
There is a significant partial autocorrelation on lag 1, 2, 3, 13:17, 24,25, and 26. When the lag is higher, there seems to be partial autocorrelation every 24 hours and every 7 days, confirming a seasonal period of 24 hours as well as a weekly seasonal period.
HOW TO CHECK FOR MORE SEASONS, YEAR OR MONTHS??
To compare with, we plot the first day, week, and month to see how the timeseries behaves.
```{r TS.compared.w.PACF}
p1 <- autoplot(ts(load_ts[1:24]), main="24 hours from baseline")+xlab("Time")+ylab("Load")
p2 <- autoplot(load_zoo[1:24*7], main="week from baseline")+xlab("Time")+ylab("Load")
p3 <- autoplot(load_zoo[1:24*7*4], main="month from baseline")+xlab("Time")+ylab("Load")
grid.arrange(p1, p2, p3,  ncol = 2, layout_matrix = rbind(c(1,2), c(3,3)))
```
# Load each month and outliers
```{r chunk 8}
N_obs <- sum(size_year)
D <- matrix(data=NA,nrow=N_obs,ncol=12)
colnames(D) <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
years = c("2004-", "2005-", "2006-", "2007-", "2008-", "2009-") #2009 ekki Ã­ datasetti
months = c("01-", "02-", "03-", "04-", "05-", "06-", "07-", "08-", "09-", "10-", "11-", "12-", "01-")
MaxIndex <- 1
for (y in 1:5){
year <- years[y]
for (m in 1:12){
MaxIndex_month <- 1
month <- months[m]
nextmonth <- months[m+1]
if (m == 12)
{
nextyear <- years[y+1]
temp_start <-  paste(c(year, month, "01"), collapse = "")
temp_end  <-  paste(c(nextyear, nextmonth, "01"), collapse = "")
}
else{
temp_start <-  paste(c(year, month, "01"), collapse = "")
temp_end  <-  paste(c(year, nextmonth, "01"), collapse = "")
}
sDay <- temp_start
eDay <-temp_end
monthData <- coredata(window(load_zoo, start = as.POSIXct(sDay), end = as.POSIXct(eDay)))
lengthMonth <- length(monthData)
D[MaxIndex:(MaxIndex+lengthMonth-1),m] <- monthData[1:lengthMonth]
if (lengthMonth > MaxIndex_month) {MaxIndex_month <- lengthMonth}
}
MaxIndex <- MaxIndex_month + MaxIndex
}
#Now boxplot with D
boxplot(D/1000000,
main = "Total energy demand per hour",
na.action = NULL,
xlab = "Months",
ylab = "Load (GW)",
col = "orange",
border = "black")
```
```{r days}
N <- ceiling(length(load_zoo)/24)
D <- matrix(data = NA, ncol = 24, nrow = N)
colnames(D) <- as.character(1:24)
counter <- 1
for (i in 1:(N)){
for (j in 1:24){
D[i,j] <- coredata(load_zoo[counter])
counter = counter + 1
}
}
#Now boxplot with D
boxplot(D/1000000,
main = "Total energy demand per hour",
na.action = NULL,
xlab = "Hours",
ylab = "Load (GW)",
col = "orange",
border = "black")
```
```{r years}
N_obs <- max(size_year)+1
D <- matrix(data=NA,nrow=N_obs,ncol=5)
colnames(D) <- c("2004", "2005", "2006", "2007", "2008")
years <-  c("2004-", "2005-", "2006-", "2007-", "2008-", "2009-")
for (y in 1:5){
year <- years[y]
sDay <-  paste(c(year, "01-", "01"), collapse = "")
eDay  <-  paste(c(years[y+1], "01-", "01"), collapse = "")
yearData <- coredata(window(load_zoo, start = as.POSIXct(sDay), end = as.POSIXct(eDay)))
lengthYear <- length(yearData)
D[(1:lengthYear),y] <- yearData[1:lengthYear]
}
#Now boxplot with D
boxplot(D/1000000,
main = "Total energy demand per hour",
na.action = NULL,
xlab = "Months",
ylab = "Load (GW)",
col = "orange",
border = "black")
```
# Transformation
Does not improve our results
```{r chunk 9}
Transformation <- function(TS, Lambda)
{
w <- vector()
for(i in 1:length(TS)){
if (Lambda == 0){
w[i] <- log(TS[i])
}
else{
w[i] <- (((TS[i])^Lambda)-1)/Lambda
}
}
return(w)
}
BackTransform <- function(TS, Lambda){
w <- vector()
for(i in 1:length(TS)){
if (Lambda == 0){
w[i] <- exp(TS[i])
}
else{
w[i] <- (Lambda*TS[i]+1)^(1/Lambda)
}
}
return(w)
}
Lambda <- c(-1, 0, 1, 2)*2
for (i in Lambda){
TS_sum_transformed <- Transformation(TS_sum, i)
plot.zoo(TS_sum_transformed)
}
Lambda <- BoxCox.lambda(TS_sum)
TS_sum_transformed <- Transformation(TS_sum, Lambda)
plot.zoo(TS_sum_transformed)
TS_sum_backtransformed <- BackTransform(TS_sum_transformed, Lambda)
plot.zoo(TS_sum_backtransformed)
```
Complete_TS.csv
Complete_TS.csv
TS <- read.csv("./../../Data/Complete_TS.csv")
TS <- read.csv("./../../Data/Complete_TS.csv")
TST <- read.csv("./../..//Data/TST.csv")
head(TS)
library(fields)
library(gplots)
dataLoad <- function(){
TS <- read.csv("./../../Data/Complete_TS.csv")
TST <- read.csv("./../../Data/TST.csv")
row.names(TST) <- TST$datetime
TST = subset(TST, select = -c(X, datetime))
row.names(TS) <- TS$datetime
TS = subset(TS, select = -c(X, datetime, V10))  #Remove Z10 which is V11 because it behaves strangely
colnames(TS) <- c("L1", "L2", "L3", "L4", "L5", "L6", "L7", "L8", "L10", "L11", "L12", "L13", "L14", "L15", "L16", "L17", "L18", "L19", "L20")
colnames(TST) <- c("T1", "T2", "T3", "T4", "T5", "T6", "T7", "T8", "T9", "T10", "T11")
return(list(TS, TST))
}
correlation <- function(TS, TST){
D <- matrix(data=NA, nrow = dim(TS)[2], ncol = dim(TST)[2])
for(i in 1:dim(TS)[2]){ #The rows are i
for(j in 1:dim(TST)[2]){ #The columns are j
x <- TS[,i]
y <- TST[,j]
#temp <-  cor(x,y)
D[i,j] <-cor(x,y)#temp$distance
}
}
return(D)
}
#temperature data
temp <- dataLoad()
TS <- as.matrix(temp[[1]])
TST <- as.matrix(temp[[2]])
TST <- TST[1:39412,] #remove last na values in TST
D <- matrix(data=NA, nrow = dim(TST)[2], ncol = dim(TST)[2])
for (i in 1:(dim(TST)[2])){
for (j in 1:(dim(TST)[2])){
temp1 <- TST[,i]
temp2 <- TST[,j]
D[i,j] <- cor(temp1, temp2)
}
}
par(mfrow=c(1,2))
colors <- colorpanel(100, "white", "grey", "black")
N<- length(colors)
breaks <- (seq(0, max(D),  length.out= N+1))
image.plot(x=1:dim(D)[1], y=1:dim(D)[2], ((D)), col = colors, breaks=breaks,
xlab="Zones", ylab="Zones",
main = "Correlation of temperature stations")
#load data
temp <- dataLoad()
TS <- as.matrix(temp[[1]])
TST <- as.matrix(temp[[2]])
TS <- TS[1:39575,] #remove last na values in TST
D <- matrix(data=NA, nrow = dim(TS)[2], ncol = dim(TS)[2])
for (i in 1:(dim(TS)[2])){
for (j in 1:(dim(TS)[2])){
temp1 <- TS[,i]
temp2 <- TS[,j]
D[i,j] <- cor(temp1, temp2)
}
}
colors <- colorpanel(100, "white","grey", "black")
N<- length(colors)
breaks <- (seq(0, max(D),  length.out= N+1))
image.plot(x=1:dim(D)[1], y=1:dim(D)[2], ((D)), col = colors, breaks=breaks,
xlab="Zones", ylab="Zones",
main = "Correlation of load stations")
library(fields)
library(gplots)
dataLoad <- function(){
TS <- read.csv("./../../Data/Complete_TS.csv")
TST <- read.csv("./../../Data/TST.csv")
row.names(TST) <- TST$datetime
TST = subset(TST, select = -c(X, datetime))
row.names(TS) <- TS$datetime
TS = subset(TS, select = -c(X, datetime, V10))  #Remove Z10 which is V11 because it behaves strangely
colnames(TS) <- c("L1", "L2", "L3", "L4", "L5", "L6", "L7", "L8", "L10", "L11", "L12", "L13", "L14", "L15", "L16", "L17", "L18", "L19", "L20")
colnames(TST) <- c("T1", "T2", "T3", "T4", "T5", "T6", "T7", "T8", "T9", "T10", "T11")
return(list(TS, TST))
}
Normalize <- function(TS, TST){
#library(energy) #for dcor
TS_mean <- colMeans(TS) #Mean of each Zone for standardization
TST_mean <- colMeans(TST)
TS_std <- apply(TS, 2, sd)
TST_std <-  apply(TST, 2, sd)
TST_standardized <- matrix(data=NA, nrow = dim(TST)[1], ncol =  dim(TST)[2])
for (i in 1:dim(TST)[1]){
for (j in 1:dim(TST)[2]){
TST_standardized[i,j] <- (TST[i,j]-TST_mean[j])/TST_std[j]
}
}
TS_standardized <- matrix(data=NA, nrow = dim(TS)[1], ncol =  dim(TS)[2])
for (i in 1:dim(TS)[1]){
for (j in 1:dim(TS)[2]){
TS_standardized[i,j] <- (TS[i,j]-TS_mean[j])/TS_std[j]
}
}
return(list(TS_standardized, TST_standardized))
}
correlation <- function(TS, TST){
D <- matrix(data=NA, nrow = dim(TS)[2], ncol = dim(TST)[2])
for(i in 1:dim(TS)[2]){ #The rows are i
for(j in 1:dim(TST)[2]){ #The columns are j
x <- TS[,i]
y <- TST[,j]
#temp <-  cor(x,y)
D[i,j] <-cor(x,y)#temp$distance
}
}
return(D)
}
correlationPlot <- function(s, last, TS, TST, title){
TS <- TS[(s+1):(last+s),]
TST <- TST[1:last,]
#Normalize
#temp <- Normalize(TS, TST)
#TS <- as.matrix(temp[[1]])
#TST <- as.matrix(temp[[2]])
C <- correlation(TS, TST)
colors <- colorpanel(100, "red","white", "blue")
N<- length( colors)
breaks <- seq(min(C), max(C),  length.out= N+1 )
image.plot(x=1:dim(C)[1], y=1:dim(C)[2], ((C)), col = colors, breaks=breaks,
xlab="Load Zones", ylab="Temperature Zones",
main = title)
}
temp <- dataLoad()
TS <- temp[[1]]
TST <- temp[[2]]
s <- 0
last <- 15000 #
L <- c(0, 6, 12, 18)
counter = 1
mustore <- vector(length = length(L))
titles <- c("Correlation with no lag", "Correlation with lag 6","Correlation with lag 12","Correlation with lag 18")
par(mfrow=c(2,2))
for (s in L){
mu <- correlationPlot(s, last, TS, TST, titles[counter])
counter = counter + 1
}
TS <- read.csv("./../../Data/Complete_TS.csv")
TST <- read.csv("./../../Data/TST.csv")
TST = subset(TST, select = -c(X, datetime))
TS = subset(TS, select = -c(X, datetime, V10))  #Remove Z10 which is V11 because it behaves strangely
TS_sum <- rowSums(TS[(1:dim(TS)[1]), ])
TST_mean <- rowMeans(TST[(1:dim(TS)[1]), ])
n <- 15000
TST_mean <- as.matrix(TST_mean[1:n])
TS_sum <- as.matrix(TS_sum[1:n])
counter <- 1
Imax <- 24*2
C <- matrix(data=NA, nrow = Imax+1, ncol = 2)
for (i in (0:(length(C)-1))){
TS_sum_lagged <- as.matrix(TS_sum[(i+1):length(TS_sum)])
TST_mean_lagged <- as.matrix(TST_mean[1:(length(TST_mean)-i)])
C[counter,1] <- i
C[counter,2] <- cor(TST_mean_lagged, TS_sum_lagged)
counter = counter + 1
}
C <- as.data.frame(C)
colnames(C) <- c("Lag", "Correlation")
library(ggplot2)
ggplot(C, aes(x=Lag, y=Correlation)) +
theme_bw() +
geom_point() +
labs(title="Correlation betw. temperature and load")
TS <- read.csv("./../../Data/Complete_TS.csv")
TST <- read.csv("./../../Data/TST.csv")
row.names(TST) <- TST$datetime
TST = subset(TST, select = -c(X, datetime))
row.names(TS) <- TS$datetime
TS = subset(TS, select = -c(X, datetime))
#STANDARDIZE
Z <- t(as.matrix(TS))
TSstandardized <-  t(scale(Z, center = TRUE, scale = TRUE))
Z <- t(as.matrix(TST))
TSTstandardized <- t(scale(Z, center = TRUE, scale = TRUE))
TSmeans <- rowMeans(TS[1:29414, ])
TSmeans[1]
TSmeans_standardized <- rowMeans(TSstandardized[1:29414, ])
TSTmeans <- rowMeans(TST[1:29414, ])
TSTmeans[1]
TSTmeans_standardized <- rowMeans(TSTstandardized[1:29414, ])
data <- as.data.frame(cbind(TSTmeans, TSmeans))
data_standardized <- as.data.frame(cbind(TSTmeans_standardized, TSmeans_standardized))
plot(TSTmeans, TSmeans,
main = "Relationship between energy load and temperature",
xlab = "Temperature [F]",
ylab = "Load [xW]",
cex = 0.1,
panel.first = grid(nx = NULL, ny = NULL, col = "red", lty = "dotted"))
plot(TSTmeans_standardized, TSmeans_standardized,
main = "Relationship between energy load and temperature (standardized)",
xlab = "Temperature",
ylab = "Load",
cex = 0.1,
panel.first = grid(nx = NULL, ny = NULL, col = "red", lty = "dotted"))
#Try divide each value with the biggest value of each column and plotting again
Z <- as.matrix(TS)
colMax <- function(X) apply(na.omit(X), 2, max)
Zmax <- as.numeric(colMax(Z))
mapply(`/`, data.frame(a), b)
c <- apply(Z, 2,  function(x) as.numeric(x) / Zmax)
library(zoo)
times <- as.POSIXct(row.names(data), format = "%Y-%m-%d %H:%M:")
timesNAomit <- na.omit(times)
load.ts <- as.zoo(
x         = data$TSmeans,
order.by  = timesNAomit,
frequency = 24
)
temperature.ts <- zoo(
x         = data$TSTmeans,
order.by  = timesNAomit,
frequency = 24
)
plotData <- cbind(load.ts, temperature.ts)
library(ggplot2)
ggplot(plotData, aes(timesNAomit)) +
geom_line(aes(y = load.ts, colour = "Load")) +
geom_line(aes(y = temperature.ts, colour = "Temperature"))+
labs(title= "Correlation of load and temperature",
y="Value", x = "Year")
#_____________________ Create and work with TS____________________________#
library(forecast)
library(xts)
TS <- read.csv("../Data/TS.csv")
row.names(TS) <- TS$datetime
TS <- TS[,3:22]
colnames(TS) <-  c("Z1", "Z2", "Z3", "Z4", "Z5", "Z6", "Z7", "Z8", "Z9",
"Z10", "Z11", "Z12" ,"Z13", "Z14", "Z15", "Z16", "Z17", "Z18", "Z19", "Z20")
TS <- TS[1]
#TS
startDay <- as.POSIXct(row.names(TS)[1], format = "%Y-%m-%d %H:%M:")
load.ts <- ts(TS, start = as.numeric(startDay))
#ZOO
times <- as.POSIXct(row.names(TS), format = "%Y-%m-%d %H:%M:")
load.zoo <- as.zoo(
x         = TS,
order.by  = times,
frequency = 24
)
#autoplot(load.zoo)
#autoplot(load.zoo, facets = FALSE)
gglagplot(load.ts, do.lines=FALSE)
ggAcf(load.ts, type = "correlation")
ggAcf(load.ts, type = "partial")
ggAcf(TS)
g <- ggAcf(load.ts, plot=FALSE)
plot(diff(load_xts))
j
8
j=9
j
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="Autocorrelations for ....")+theme_bw()
ggAcf(p, type = "partial") + labs(title="Autocorrelations for ....")+theme_bw()
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari2.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="Autocorrelations for ....")+theme_bw()
ggAcf(p, type = "partial") + labs(title="Autocorrelations for ....")+theme_bw()
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="ari1")+theme_bw()
ggAcf(p, type = "partial") + labs(title="ari1")+theme_bw()
p <- read.csv("./../../Data/Predict & Hyper/ari2.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="ari2")+theme_bw()
ggAcf(p, type = "partial") + labs(title="ari2")+theme_bw()
p <- read.csv("./../../Data/Predict & Hyper/ari3.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="ari3")+theme_bw()
ggAcf(p, type = "partial") + labs(title="ari3")+theme_bw()
p <- read.csv("./../../Data/Predict & Hyper/ari4.csv", header = F)
p = p[!is.na(p)]
ggAcf(p) + labs(title="ari4")+theme_bw()
ggAcf(p, type = "partial") + labs(title="ari4")+theme_bw()
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="ari1")+theme_bw()
p2 <- ggAcf(p, type = "partial") + labs(title="ari1")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="Autocorrelation of predicted values")+theme_bw()
p2 <- ggAcf(p, type = "Partial") + labs(title="Partial autocorrelation of predicted values")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="Autocorrelation of predicted values")+theme_bw()
p2 <- ggAcf(p, type = "Partial") + labs(title="Partial autocorrelation of predicted values")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="Autocorrelation of predicted values")+theme_bw()
p2 <- ggAcf(p, type = "Partial") + labs(title="Partial autocorrelation of predicted values")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="Autocorrelation of predicted values")+theme_bw()
p2 <- ggAcf(p, type = "Partial") + labs(title="Partial autocorrelation of predicted values")+theme_bw()
library(forecast)
library(ggplot2)
#set current directory to code source
p <- read.csv("./../../Data/Predict & Hyper/ari1.csv", header = F)
p = p[!is.na(p)]
p1 <- ggAcf(p) + labs(title="Autocorrelation of predicted values")+theme_bw()
p2 <- ggAcf(p, type = "partial") + labs(title="Partial autocorrelation of predicted values")+theme_bw()
grid.arrange(p1, p2, nrow = 2)
